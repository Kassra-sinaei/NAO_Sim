{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "democratic-swedish",
   "metadata": {},
   "source": [
    "## Full Body Trajectory for Walking\n",
    "**Implmentation of an A2C Agent for learning Biped Gait** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-boulder",
   "metadata": {},
   "source": [
    "Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-harris",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet\n",
    "import time\n",
    "import pybullet_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "from qibullet import SimulationManager\n",
    "from qibullet import NaoVirtual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-dream",
   "metadata": {},
   "source": [
    "GYM Based Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-literature",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "    TODO:\n",
    "        0-Check Step Time and IMU Frequency\n",
    "        1-Calculate Linear velocity of pelvis from IMU Data\n",
    "        2-Complete observe()\n",
    "        3-Form Reward Function \n",
    "\"\"\"\n",
    "\n",
    "class NAO(gym.Env):\n",
    "    #metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self,imu_freq = 240,max_episode_legth = 10000):\n",
    "        super(SelfBalancing, self).__init__()\n",
    "        # Define action and observation space\n",
    "        self.action_space = spaces.Box(low=np.array([-1.53589,-1.14529,-0.379435,-0.0923279,-1.18944,-0.397761, \n",
    "                                                    -1.53589,-1.14529,-0.79046,-0.0923279,-1.1863,-0.768992]),\n",
    "                                       high=np.array([0.48398,0.740718,0.79046,2.11255,0.922581,0.768992,\n",
    "                                                     0.48398,0.740718,0.379435,2.11255,0.932006,0.397761]),\n",
    "                                       dtype=np.float64)\n",
    "        self.observation_space = spaces.Box(low=np.array([-10,-10,-10,-10]), \n",
    "                                            high=np.array([10,10,10,10]),\n",
    "                                           dtype=np.float64) \n",
    "        \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "            Action Space: 12 component for lower limb joint positions\n",
    "                          LHipPitch,LHipPitchYaw,LHipRoll,LkneePitch,LAnklePitch,LAnkleRoll\n",
    "                          RHipPitch,RHipPitchYaw,RHipRoll,RkneePitch,RAnklePitch,RAnkleRoll\n",
    "        \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "            Observation Space: LPelvis,RPelvis Poition (z component), \n",
    "                               Torso Orientation (y component),\n",
    "                               Toros's linear velocity (normal x & y component)\n",
    "        \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "        self.max_steps = max_episode_length\n",
    "        self.steps = 0\n",
    "        self.imu_freq = imu_freq\n",
    "        \n",
    "        self.velocity_x = 0\n",
    "        self.simulation_manager,self.phisycsClient,self.nao, self.imu = self.reset()\n",
    "        self.effort = 1.0\n",
    "        \n",
    "    def step(self,action):\n",
    "        self.take_action(action)\n",
    "        obs = self.observe()\n",
    "        reward = self.calculate_reward(obs)\n",
    "        done = self.is_terminated()\n",
    "        return obs, reward, done, {}\n",
    "    \n",
    "    def take_action(self,action):\n",
    "        self.nao.setAngles([\"LHipPitch\",\"LHipPitchYaw\",\"LHipRoll\",\"LkneePitch\",\"LAnklePitch\",\"LAnkleRoll\", ,\n",
    "                            \"RHipPitch\",\"RHipPitchYaw\",\"RHipRoll\",\"RkneePitch\",\"RAnklePitch\",\"RAnkleRoll\"],\n",
    "                          action, [self.effort]*12)\n",
    "        pybullet.stepSimulation()\n",
    "        pass\n",
    "    \n",
    "    def calculate_reward(self,obs):\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def observe(self):\n",
    "        _, linear_acceleration = self.nao.getImuValues()\n",
    "        a = linear_aceleration[0]\n",
    "        self.velocity_x += (1/self.imu_freq) * a \n",
    "        \n",
    "        torsoYaw = pybullet.getEulerFromQuaternion(nao.getLinkPosition(\"torso\")[1])[2]\n",
    "        torsoPitch = pybullet.getEulerFromQuaternion(nao.getLinkPosition(\"torso\")[1])[1]\n",
    "        torsoRoll = pybullet.getEulerFromQuaternion(nao.getLinkPosition(\"torso\")[1])[0]\n",
    "        \n",
    "        pelvisHeight = 0.5*(self.nao.getLinkPosition([\"LPelvis\"])[0][2] + self.nao.getLinkPosition([\"RPelvis\"])[0][2])  \n",
    "        return np.array([self.velocity_x,torso_Yaw , torsoPitch, torsoRoll, PelvisHeight])\n",
    "    \n",
    "    def is_terminated():\n",
    "        if self.steps == self.max_steps:\n",
    "            self.steps = 0\n",
    "             self.simulation_manager,self.phisycsClient,self.nao, self.imu = self.reset()\n",
    "            return True\n",
    "        elif #robot colapses:\n",
    "            self.simulation_manager,self.phisycsClient,self.nao, self.imu = self.reset()\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def reset(self):\n",
    "        # Close Previous Environment\n",
    "        simulation_manager.stopSimulation(phisycsClient)\n",
    "        \n",
    "        #Instantiate Phisycs Client\n",
    "        phisycsClient = pybullet.connect(pybullet.GUI)\n",
    "        pybullet.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "        \n",
    "        #Spawn Robot\n",
    "        simulation_manager = SimulationManager()\n",
    "        simulation_manager.setGravity(phisycsClient, [0.0, 0.0, -9.81])\n",
    "        nao = simulation_manager.spawnNao(phisycsClient, spawn_ground_plane=True) \n",
    "        pybullet.setRealTimeSimulation(0)\n",
    "        imu = nao.getImu()\n",
    "        nao.subscribeImu(frequency=self.imu_freq)\n",
    "        self.velocity_x = 0\n",
    "        return simulation_manager,phisycsClient,nao, imu\n",
    "    \n",
    "    def __close__(self):\n",
    "        self.simulation_manager.stopSimulation(self.phisycsClient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-pittsburgh",
   "metadata": {},
   "source": [
    "A2C Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-music",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import coax\n",
    "import optax\n",
    "import haiku as hk\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from numpy import prod\n",
    "\n",
    "# pick environment\n",
    "name = 'A2C'\n",
    "env = NAO()\n",
    "env = coax.wrappers.TrainMonitor(env, name=name, tensorboard_dir=f\"./data/tensorboard/{name}\") \n",
    "\n",
    "def func_v(S, is_training):\n",
    "    # custom haiku function\n",
    "    value = hk.Sequential([\n",
    "                          hk.Linear(20),\n",
    "                          hk.Linear(20),\n",
    "                          hk.Linear(1,w_init=jnp.zeros),jnp.ravel])\n",
    "    return value(S)  # output shape: (batch_size,)\n",
    "\n",
    "def func_pi(S, is_training):\n",
    "    shared = hk.Sequential((\n",
    "        hk.Linear(20), jax.nn.relu,\n",
    "        hk.Linear(20), jax.nn.relu,\n",
    "    ))\n",
    "    mu = hk.Sequential((\n",
    "        shared,\n",
    "        hk.Linear(10), jax.nn.relu,\n",
    "        hk.Linear(3, w_init=jnp.zeros),\n",
    "        hk.Reshape(env.action_space.shape),\n",
    "    ))\n",
    "    logvar = hk.Sequential((\n",
    "        shared,\n",
    "        hk.Linear(8), jax.nn.relu,\n",
    "        hk.Linear(3, w_init=jnp.zeros),\n",
    "        hk.Reshape(env.action_space.shape),\n",
    "    ))\n",
    "    return {'mu': mu(S), 'logvar': logvar(S)}\n",
    "\n",
    "# function approximators\n",
    "v = coax.V(func_v, env)\n",
    "pi = coax.Policy(func_pi, env)\n",
    "\n",
    "\n",
    "# specify how to update policy and value function\n",
    "vanilla_pg = coax.policy_objectives.VanillaPG(pi, optimizer=optax.adam(0.001))\n",
    "simple_td = coax.td_learning.SimpleTD(v, optimizer=optax.adam(0.002))\n",
    "\n",
    "\n",
    "# specify how to trace the transitions\n",
    "tracer = coax.reward_tracing.NStep(n=5, gamma=0.9)\n",
    "buffer = coax.experience_replay.SimpleReplayBuffer(capacity=256)\n",
    "\n",
    "\n",
    "for ep in range(10):\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(env.max_episode_steps):\n",
    "        a, logp = pi(s, return_logp=True)\n",
    "        s_next, r, done, info = env.step(a)\n",
    "\n",
    "        # add transition to buffer\n",
    "        # N.B. vanilla-pg doesn't use logp but we include it to make it easy to\n",
    "        # swap in another policy updater that does require it, e.g. ppo-clip\n",
    "        tracer.add(s, a, r, done, logp)\n",
    "        while tracer:\n",
    "            buffer.add(tracer.pop())\n",
    "\n",
    "        # update\n",
    "        if len(buffer) == buffer.capacity:\n",
    "            for _ in range(4 * buffer.capacity // 32):  # ~4 passes\n",
    "                transition_batch = buffer.sample(batch_size=32)\n",
    "                metrics_v, td_error = simple_td.update(transition_batch, return_td_error=True)\n",
    "                metrics_pi = vanilla_pg.update(transition_batch, td_error)\n",
    "                env.record_metrics(metrics_v)\n",
    "                env.record_metrics(metrics_pi)\n",
    "\n",
    "            buffer.clear()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        s = s_next\n",
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-suite",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
